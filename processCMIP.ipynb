{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "\n",
    "import glob\n",
    "from cdo import *\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import cftime\n",
    "\n",
    "sys.path.append('/glade/u/home/currierw/cmip_ingest/scripts')\n",
    "from download import get_dataset\n",
    "sys.path.append('/glade/u/home/currierw/cmip_work')\n",
    "import cmipFunctions\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "cdo = Cdo()\n",
    "\n",
    "# Import state vector file using geopandas\n",
    "states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\n",
    "states_gdf = gpd.read_file(states_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Slice/Subset data down further based on latitutde (-90 -- +90), longitude (0 -- 360)\n",
    "\n",
    "#####################################################\n",
    "############## ADDITIONAL INFORMATION ############## \n",
    "#####################################################\n",
    "\n",
    "#                   [{\"var_name\":\"va\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ua\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ta\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ps\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"hus\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"prc\", \"domain\":\"atmos\", \"interval\":\"3hr\"},\n",
    "#                   {\"var_name\":\"tos\", \"domain\":\"ocean\", \"interval\":\"day\"},\n",
    "#                   {\"var_name\":\"orog\", \"domain\":\"atmos\", \"interval\":\"fx\"},\n",
    "#                   {\"var_name\":\"sftlf\", \"domain\":\"atmos\", \"interval\":\"fx\"},\n",
    "\n",
    "# DESIRED MODEL LIST\n",
    "# GCMList=['ACCESS1-0','ACCESS1-3','bcc-csm1-1','bcc-csm1-1-m','BNU-ESM','CanESM2','CCSM4','CESM1-BGC', \\\n",
    "#     'CESM1-CAM5','CMCC-CM','CMCC-CMS','CNRM-CM5','CSIRO-Mk3-6-0','EC-EARTH','FGOALS-g2','FIO-ESM', \\\n",
    "#     'GFDL-CM3','GFDL-ESM2G','GFDL-ESM2M','GISS-E2-H-cc','GISS-E2-H','GISS-E2-R-cc','GISS-E2-R', \\\n",
    "#     'GISS-E2-R','HadGEM2-AO','HadGEM2-CC','HadGEM2-ES','HadGEM2-ES','inmcm4','IPSL-CM5A-LR','IPSL-CM5A-LR', \\\n",
    "#     'IPSL-CM5A-LR','IPSL-CM5A-LR','IPSL-CM5A-MR','IPSL-CM5B-LR','MIROC-ESM','MIROC-ESM-CHEM','MIROC5', \\\n",
    "#     'MPI-ESM-LR','MPI-ESM-MR','MRI-CGCM3','NorESM1-M']\n",
    "\n",
    "# MODEL LIST ON CHEYENNE\n",
    "# chyModelList=['BCC/bcc-csm1-1','BCC/bcc-csm1-1-m','BNU/BNU-ESM','CCCma/CanESM2','CNRM-CERFACS/CNRM-CM5','CSIRO-BOM/ACCESS1-0','CSIRO-BOM/ACCESS1-3', \\\n",
    "#               'CSIRO-QCCCE/CSIRO-Mk3-6-0','IPSL/IPSL-CM5A-LR','IPSL/IPSL-CM5A-MR','IPSL/IPSL-CM5B-LR', \\\n",
    "#               'LASG-CESS/FGOALS-g2','MIROC/MIROC5','MIROC/MIROC-ESM-CHEM','MIROC/MIROC-ESM','MOHC/HadGEM2-ES','MRI/MRI-CGCM3', \\\n",
    "#               'NCC/NorESM1-M','NOAA-GFDL/GFDL-CM3','NOAA-GFDL/GFDL-ESM2G']\n",
    "\n",
    "#####################################################\n",
    "############## CHANGE THIS INFORMATION ############## \n",
    "#####################################################\n",
    "\n",
    "Institute   = 'BCC'\n",
    "Model       = 'bcc-csm1-1'  # ACCESS1-0\n",
    "time_period = 'historical'  # historical, rcp45, rcp85\n",
    "ensemble    = 'r1i1p1'      # r1i1p1\n",
    "version     = 'v1'\n",
    "lat_bnds, lon_bnds = [22, 58], [230, 265]\n",
    "\n",
    "start_date = \"19500101\"\n",
    "end_date   = \"20060101\"\n",
    "startDates = pd.to_datetime(['1950-01-01 12:00:00','1960-01-01 06:00:00','1970-01-01 06:00:00','1980-01-01 06:00:00','1990-01-01 06:00:00','2000-01-01 06:00:00'],\\\n",
    "                       format='%Y-%m-%d %H:%M:%S')\n",
    "endDates   = pd.to_datetime(['1960-01-01 00:00:00','1970-01-01 00:00:00','1980-01-01 00:00:00','1990-01-01 00:00:00','2000-01-01 00:00:00','2006-01-01 00:00:00'],\\\n",
    "                       format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#####################################################\n",
    "############## NO NEED TO CHANGE BELOW ############## \n",
    "#####################################################\n",
    "\n",
    "# Check if we've already created a directory for subsetted/processed data\n",
    "outDir = '/glade/scratch/currierw/'+Model+'/'+time_period+'/' # must have back slash\n",
    "if os.path.isdir(outDir) == False : # Make output directory if it doesn't exist\n",
    "    os.makedirs(outDir)\n",
    "    print(\"created out directory: \"+outDir)\n",
    "print(\"Out directory already exists: \"+outDir)\n",
    "\n",
    "# See if data exists on Cheyenne already\n",
    "# Note: .txt file is important for figuring this out - just searches .txt function\n",
    "#       find /glade/collections/cmip/cmip5/output1/*/*/*/6hr/atmos/6hrLev/r1i1p1/*/* -type d > .txt\n",
    "fDirs,varDir,varFiles = cmipFunctions.searchChyColl(\"/glade/u/home/currierw/cmip_work/CMIP5historical_r1i1p1.txt\",Model,'ta') # var not important here, use fDirs\n",
    "if len(fDirs)==0:\n",
    "    print('Not on Cheyenne: Need to download all variables')\n",
    "    chyColl     = False         # Data exist in cheyenne collection already - see list\n",
    "else:\n",
    "    print('Data are already on Cheyenne! Yay!')\n",
    "    chyColl     = True         # Data exist in cheyenne collection already - see list\n",
    "    inDir       = os.path.dirname(fDirs[0])+'/' # need backward slash at end\n",
    "print(\"Cheyenne data are located here: \"+inDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF FILES EXIST\n",
    "cmipFunctions.checkFiles(inDir,'hus',time_period,chyColl)\n",
    "cmipFunctions.checkFiles(inDir,'ps',time_period,chyColl)\n",
    "cmipFunctions.checkFiles(inDir,'ta',time_period,chyColl)\n",
    "cmipFunctions.checkFiles(inDir,'ua',time_period,chyColl)\n",
    "cmipFunctions.checkFiles(inDir,'va',time_period,chyColl)\n",
    "# cmipFunctions.convertTos(inDir,outDir,Model,time_period,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'tos',time_period,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TEMPERATURE\n",
    "# Load in Temperature Data First\n",
    "taFiles=glob.glob(outDir+'ta*.nc')\n",
    "if len(taFiles) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'ta',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### SEA SURFACE TEMEPRATURE\n",
    "\n",
    "# daily, historical, rcp45, rcp85, sea surface temperature data doesn't exist on cheyenne - download first\n",
    "tosFiles=glob.glob(outDir+'tos*.nc')\n",
    "if len(tosFiles) == 0 :\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=Model,var_name=\"tos\",domain=\"ocean\",interval=\"day\", run=ensemble, scenario=time_period, start_time=start_date, end_time=end_date)\n",
    "# Convert the TOS data from rotated pole to temperature data\n",
    "tosFilesRegrd=glob.glob(outDir+'tos_6hr*_regrd.nc')\n",
    "if len(tosFilesRegrd):\n",
    "    cmipFunctions.convertTos(outDir,inDir,chyColl)\n",
    "    \n",
    "# Convert the\n",
    "tosFilesSub=glob.glob(outDir+'tos_6hr*_regrd_subset.nc')\n",
    "if len(tosFilesSub) == 0:\n",
    "    cmipFunctions.processTOS(outDir,outDir,'tos',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Humidity Data\n",
    "husFiles=glob.glob(outDir+'hus*.nc')\n",
    "if len(husFiles) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'hus',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds=xr.open_dataset('/glade/collections/cmip/cmip5/output1/BCC/bcc-csm1-1/historical/6hr/atmos/6hrLev/r1i1p1/v1/ua/ua_6hrLev_bcc-csm1-1_historical_r1i1p1_198101010000-198112311800.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### U,V Data\n",
    "uFiles=glob.glob(outDir+'ua*.nc')\n",
    "if len(uFiles) == 0:\n",
    "    cmipFunctions.loadStaggeredVars(inDir,outDir,'ua',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "vFiles=glob.glob(outDir+'va*.nc')\n",
    "if len(vFiles) == 0:\n",
    "    cmipFunctions.loadStaggeredVars(inDir,outDir,'va',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Surface Pressure\n",
    "psFiles=glob.glob(outDir+'ps*.nc')\n",
    "if len(psFiles) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'ps',Model,time_period,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Precipitation\n",
    "# daily, historical, rcp45, rcp85, sea surface temperature data doesn't exist on cheyenne - download first\n",
    "prcFiles=glob.glob(outDir+'prc*.nc')\n",
    "if len(prcFiles) == 0 :\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=Model,var_name=\"prc\",domain=\"atmos\",interval=\"3hr\", run=ensemble, scenario=time_period, start_time=start_date, end_time=end_date)\n",
    "\n",
    "start = ['1960-01-01 06:00:00','1970-01-01 06:00:00','1980-01-01 06:00:00','1990-01-01 06:00:00']\n",
    "end   = ['1970-01-01 00:00:00','1980-01-01 00:00:00','1990-01-01 00:00:00','2000-01-01 00:00:00']\n",
    "startDatesPrc = pd.to_datetime(start, format='%Y-%m-%d %H:%M:%S')\n",
    "endDatesPrc   = pd.to_datetime(end,   format='%Y-%m-%d %H:%M:%S')\n",
    "# Precipitation is 3D, only time,lat,lon - stored in longer time files in 3 hr data\n",
    "prcFiles=glob.glob(outDir+'prc*subset.nc')\n",
    "if len(prcFiles) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVarsResample(outDir,outDir,'prc',Model,time_period,ensemble,lat_bnds,lon_bnds,startDatesPrc,endDatesPrc,False,'6hr','sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset('/glade/scratch/currierw/bcc-csm1-1/historical/prc_3hr_bcc-csm1-1_historical_r1i1p1_198001010130-199912312230.nc')\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = ds['prc'][0,:,:].plot(x='lon', y='lat',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()})\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsT    = xr.open_mfdataset(outDir+'ta*subset.nc',combine='by_coords')\n",
    "dsTOS  = xr.open_mfdataset(outDir+'tos*subset.nc',combine='by_coords')\n",
    "dsHus  = xr.open_mfdataset(outDir+'hus*subset.nc',combine='by_coords')\n",
    "dsU    = xr.open_mfdataset(outDir+'ua*subset.nc',combine='by_coords')\n",
    "dsV    = xr.open_mfdataset(outDir+'va*subset.nc',combine='by_coords')\n",
    "dsPrc  = xr.open_mfdataset(outDir+'prc*subset.nc',combine='by_coords')\n",
    "dsPrcO = xr.open_mfdataset(outDir+'prc_3hr*.nc',combine='by_coords')\n",
    "dsPs   = xr.open_mfdataset(outDir+'ps*subset.nc',combine='by_coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model level heights [m] or Pressure Levels\n",
    "try:\n",
    "    z_t=dsT.lev+(dsT.b*dsT.orog)\n",
    "    print('\\nCalculating Model Level Heights')\n",
    "except:\n",
    "    P = dsT['a']*dsT['p0'] + dsT['b']*dsT['ps']\n",
    "    pressure  = True\n",
    "    elevation = False\n",
    "else:\n",
    "    elevation = True\n",
    "    pressure  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate water vapor mixing ratio (w/Qv) [kg/kg] from specific humdiity\n",
    "print('\\n Converting from Specific Humidity to Water Vapor Mixing Ratio, Qv = hus/1-hus')\n",
    "dsHus['Qv'] = dsHus['hus']/(1-dsHus['hus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating New Dataset\n",
    "print(\"\\nCreating new dataset\\n\")\n",
    "\n",
    "# Create New Datasets that's just the datasets with dimensions and coordinates\n",
    "\n",
    "######## Terrain Height\n",
    "if elevation:\n",
    "    dsOrog = xr.Dataset({\"HGT\":((\"lat\",\"lon\"),tads_sub.orog)},\n",
    "                     coords={\"lat\":tads_sub.lat,\"lon\":tads_sub.lon})\n",
    "    dsOrog['HGT'].attrs['standard_name'] = 'Terrain Height'\n",
    "    dsOrog['HGT'].attrs['long_name']     = 'Terrain Height'\n",
    "    dsOrog['HGT'].attrs['units']         = 'm'\n",
    "    dsOrog['HGT'].attrs['Processing Note'] = 'Provided from temeprature dataset - orogoraphy variable'\n",
    "    ######## 3D Model Level Heights [m]\n",
    "    ds_Z    = xr.Dataset({\"Z\":((\"lev\",\"lat\",\"lon\"),z_t)},\n",
    "                 coords={\"lev\":tads_sub.lev,\"lat\":tads_sub.lat,\"lon\":tads_sub.lon})\n",
    "    ds_Z['Z'].attrs['standard_name'] = '3D model level heights'\n",
    "    ds_Z['Z'].attrs['long_name'] = '3D Model Level Heights'\n",
    "    ds_Z['Z'].attrs['units'] = 'm' \n",
    "    ds_Z['Z'].attrs['Processing Note'] = 'Calculated using temperature lev coordinates, b values, and orogoraphy varaibles. Z = lev + b * orog' \n",
    "\n",
    "if pressure:\n",
    "    ds_P = xr.Dataset({\"P\":((\"time\",\"lev\",\"lat\",\"lon\"),P)},\n",
    "                 coords={\"time\":dsT.time,\"lev\":P.lev,\"lat\":P.lat,\"lon\":P.lon})\n",
    "    ds_P['P'].attrs['standard_name'] = 'pressure' \n",
    "    ds_P['P'].attrs['long_name'] = 'pressure' \n",
    "    ds_P['P'].attrs['comment'] = 'calculated from subsetted 6 hr temperature data file: a*p0+b*Ps' \n",
    "    ds_P['P'].attrs['units'] = 'Pa' \n",
    "\n",
    "    \n",
    "######## Surface Air Pressure [Pa]\n",
    "ds_Ps   = xr.Dataset({\"Ps\":((\"time\",\"lat\",\"lon\"),dsPs.ps)},\n",
    "                 coords={\"time\":dsPs.time,\"lat\":dsPs.lat,\"lon\":dsPs.lon})\n",
    "ds_Ps['Ps'].attrs = dsPs.ps.attrs\n",
    "\n",
    "######## Sea Surface Temeperature [K]\n",
    "ds_SST  = xr.Dataset({\"SST\":((\"time\",\"lat\",\"lon\"),dsTOS.tos)},\n",
    "                 coords={\"time\":dsTOS.time,\"lat\":dsT.lat,\"lon\":dsT.lon})\n",
    "ds_SST['SST'].attrs = dsTOS['tos'].attrs\n",
    "ds_SST['SST'].attrs['Processing Note'] = 'Daily data forward filled to 6H data' \n",
    "\n",
    "######## Precipitation [kg m-2 s-1]\n",
    "ds_prec = xr.Dataset({\"prec\":((\"time\",\"lat\",\"lon\"),dsPrc.prc)},\n",
    "                 coords={\"time\":dsPrc.time,\"lat\":dsPrc.lat,\"lon\":dsPrc.lon})\n",
    "ds_prec['prec'].attrs['standard_name'] =dsPrcO['prc'].attrs['standard_name']\n",
    "ds_prec['prec'].attrs['long_name'] = dsPrcO['prc'].attrs['long_name']\n",
    "ds_prec['prec'].attrs['comment'] = 'at surface. This is a 6-hour mean convective preciptiation'\n",
    "ds_prec['prec'].attrs['units'] = dsPrcO['prc'].attrs['units']\n",
    "ds_prec['prec'].attrs['cell_methods'] = dsPrcO['prc'].attrs['cell_methods']\n",
    "ds_prec['prec'].attrs['cell_measures'] = dsPrcO['prc'].attrs['cell_measures']\n",
    "ds_prec['prec'].attrs['associated_files'] = dsPrcO['prc'].attrs['associated_files']\n",
    "ds_prec['prec'].attrs['Processing Note'] = 'Resample from 3H data to 6H data using the sum between time-steps'\n",
    "\n",
    "######## Air Temperature [K[]]\n",
    "ds_T = xr.Dataset({\"T\":((\"time\",\"lev\",\"lat\",\"lon\"),dsT.ta)},\n",
    "                  coords={\"time\":dsT.time,\"lev\":dsT.lev,\"lat\":dsT.lat,\"lon\":dsT.lon})\n",
    "ds_T['T'].attrs = dsT.ta.attrs\n",
    "\n",
    "######## Water Vapor Mixing Ratio [kg/kg]\n",
    "ds_Qv = xr.Dataset({\"Qv\":((\"time\",\"lev\",\"lat\",\"lon\"),dsHus.Qv)},\n",
    "                   coords={\"time\":dsHus.time,\"lev\":dsHus.lev,\"lat\":dsHus.lat,\"lon\":dsHus.lon})\n",
    "ds_Qv['Qv'].attrs['standard_name'] = 'Water Vapor Mixing Ratio'\n",
    "ds_Qv['Qv'].attrs['long_name'] = 'Water Vapor Mixing Ratio'\n",
    "ds_Qv['Qv'].attrs['units'] = 'kg/kg'\n",
    "ds_Qv['Qv'].attrs['cell measures'] = dsHus['hus'].attrs['cell_measures']\n",
    "ds_Qv['Qv'].attrs['associated_files'] = dsHus['hus'].attrs['associated_files']\n",
    "ds_Qv['Qv'].attrs['Processing Note'] = 'Calculated from specific humidity (q) data Qv = q/1-q'\n",
    "\n",
    "######## North South Wind Speeds [m s-1]\n",
    "ds_v = xr.Dataset({\"V\":((\"time\",\"lev\",\"lat\",\"lon\"),dsV.va)},\n",
    "                  coords={\"time\":dsV.time,\"lev\":dsT.lev,\"lat\":dsV.lat,\"lon\":dsV.lon})\n",
    "ds_v['V'].attrs = dsV.va.attrs\n",
    "ds_v['V'].attrs['Processing Note'] = 'Interpolated/Regrided from staggered north-south grid - one additional row offset by 1/2 a grid cell in latitude to the temperature grid'\n",
    "\n",
    "######## East Wind Wind Speeds [m s-1]\n",
    "ds_u = xr.Dataset({\"U\":((\"time\",\"lev\",\"lat\",\"lon\"),dsU.ua)},\n",
    "                  coords={\"time\":dsU.time,\"lev\":dsT.lev,\"lat\":dsU.lat,\"lon\":dsU.lon})\n",
    "ds_u['U'].attrs = dsU.ua.attrs\n",
    "ds_u['U'].attrs['Processing Note'] = 'Interpolated/Regrided from staggered east-west grid - one additional column offset by 1/2 a grid cell in longitude to the temperature grid'\n",
    "                     \n",
    "# Make a new dataset\n",
    "if elevation:\n",
    "    ds = xr.merge([ds_orog, ds_Z, ds_Ps, ds_SST, ds_prec, ds_T, ds_Qv, ds_v, ds_u])\n",
    "elif pressure:\n",
    "    ds = xr.merge([ds_P, ds_Ps, ds_SST, ds_prec, ds_T, ds_Qv, ds_v, ds_u])\n",
    "    \n",
    "print(\"Created new dataset\")\n",
    "now = datetime.now()\n",
    "ds.attrs['Condensed/Merged File Created'] = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "ds.attrs = dsT.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = ds['SST'][0,:,:].plot(x='lon', y='lat',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()})\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = ds['T'][0,0,:,:].plot(x='lon', y='lat',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()})\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out files\n",
    "allFiles=glob.glob(outDir+Model+'_6hrLev_'+ time_period +'_'+ ensemble +'*' +'_subset.nc')\n",
    "if len(allFiles) == 0:\n",
    "    for startDateLs, endDateLs in zip(startDates, endDates):\n",
    "        ds_sub_Time  = ds.sel(time=slice(startDateLs.strftime('%Y-%m-%dT%H:%M:%S'),endDateLs.strftime('%Y-%m-%dT%H:%M:%S')))  # Time slice\n",
    "        ds_sub_Time.to_netcdf(outDir+Model+'_6hrLev_'+ time_period +'_'+ ensemble +'_' \\\n",
    "                                + startDateLs.strftime('%Y%m%d')+'-'+endDateLs.strftime('%Y%m%d') \\\n",
    "                                +'_subset.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=xr.open_dataset(allFiles[1])\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
