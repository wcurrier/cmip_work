{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import dask\n",
    "\n",
    "import glob\n",
    "from cdo import *\n",
    "cdo = Cdo()\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import cftime\n",
    "\n",
    "sys.path.append('/glade/u/home/currierw/cmip_ingest/scripts')\n",
    "from download import get_dataset\n",
    "sys.path.append('/glade/u/home/currierw/cmip_work')\n",
    "import cmipFunctions\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# Import state vector file using geopandas\n",
    "states_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\n",
    "states_gdf = gpd.read_file(states_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out directory already exists: /glade/scratch/currierw/GFDL-CM3/historical/\n",
      "Not on Cheyenne: Need to download all variables\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# Slice/Subset data down further based on latitutde (-90 -- +90), longitude (0 -- 360)\n",
    "\n",
    "#####################################################\n",
    "############## ADDITIONAL INFORMATION ############## \n",
    "#####################################################\n",
    "\n",
    "#                   [{\"var_name\":\"va\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ua\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ta\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"ps\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"hus\", \"domain\":\"atmos\", \"interval\":\"6hr\"},\n",
    "#                   {\"var_name\":\"prc\", \"domain\":\"atmos\", \"interval\":\"3hr\"},\n",
    "#                   {\"var_name\":\"tos\", \"domain\":\"ocean\", \"interval\":\"day\"},\n",
    "#                   {\"var_name\":\"orog\", \"domain\":\"atmos\", \"interval\":\"fx\"},\n",
    "#                   {\"var_name\":\"sftlf\", \"domain\":\"atmos\", \"interval\":\"fx\"},\n",
    "\n",
    "# DESIRED MODEL LIST\n",
    "# GCMList=['ACCESS1-0','ACCESS1-3','bcc-csm1-1','bcc-csm1-1-m','BNU-ESM','CanESM2','CCSM4','CESM1-BGC', \\\n",
    "#     'CESM1-CAM5','CMCC-CM','CMCC-CMS','CNRM-CM5','CSIRO-Mk3-6-0','EC-EARTH','FGOALS-g2','FIO-ESM', \\\n",
    "#     'GFDL-CM3','GFDL-ESM2G','GFDL-ESM2M','GISS-E2-H-cc','GISS-E2-H','GISS-E2-R-cc','GISS-E2-R', \\\n",
    "#     'GISS-E2-R','HadGEM2-AO','HadGEM2-CC','HadGEM2-ES','HadGEM2-ES','inmcm4','IPSL-CM5A-LR','IPSL-CM5A-LR', \\\n",
    "#     'IPSL-CM5A-LR','IPSL-CM5A-LR','IPSL-CM5A-MR','IPSL-CM5B-LR','MIROC-ESM','MIROC-ESM-CHEM','MIROC5', \\\n",
    "#     'MPI-ESM-LR','MPI-ESM-MR','MRI-CGCM3','NorESM1-M']\n",
    "\n",
    "# MODEL LIST ON CHEYENNE\n",
    "# chyModelList=['BCC/bcc-csm1-1','BCC/bcc-csm1-1-m','BNU/BNU-ESM','CCCma/CanESM2','CNRM-CERFACS/CNRM-CM5','CSIRO-BOM/ACCESS1-0','CSIRO-BOM/ACCESS1-3', \\\n",
    "#               'CSIRO-QCCCE/CSIRO-Mk3-6-0','IPSL/IPSL-CM5A-LR','IPSL/IPSL-CM5A-MR','IPSL/IPSL-CM5B-LR', \\\n",
    "#               'LASG-CESS/FGOALS-g2','MIROC/MIROC5','MIROC/MIROC-ESM-CHEM','MIROC/MIROC-ESM','MOHC/HadGEM2-ES','MRI/MRI-CGCM3', \\\n",
    "#               'NCC/NorESM1-M','NOAA-GFDL/GFDL-CM3','NOAA-GFDL/GFDL-ESM2G']\n",
    "\n",
    "#####################################################\n",
    "############## CHANGE THIS INFORMATION ############## \n",
    "#####################################################\n",
    "\n",
    "ins           = 'NOAA'        # BCC, NCAR, 'CNRM-CERFACS'\n",
    "model         = 'GFDL-CM3'  # bcc-csm1-1, ACCESS1-0,'CNRM-CM5'\n",
    "scen          = 'historical'  # historical, rcp45, rcp85\n",
    "ensemble      = 'r1i1p1'      # r6i1p1, r1i1p1\n",
    "# version     = 'v20121128'   # v1, v20121128\n",
    "\n",
    "lat_bnds, lon_bnds = [22, 58], [230, 265]\n",
    "# Need to write these in:\n",
    "if scen=='historical':\n",
    "    start_date = \"19500101\" # Used for downloading data\n",
    "    end_date   = \"20060101\" # Used for downloading data\n",
    "    startDates = pd.to_datetime(['1950-01-01 12:00:00','1960-01-01 06:00:00','1970-01-01 06:00:00','1980-01-01 06:00:00','1990-01-01 06:00:00','2000-01-01 06:00:00'],format='%Y-%m-%d %H:%M:%S')\n",
    "    endDates   = pd.to_datetime(['1960-01-01 00:00:00','1970-01-01 00:00:00','1980-01-01 00:00:00','1990-01-01 00:00:00','2000-01-01 00:00:00','2006-01-01 00:00:00'],format='%Y-%m-%d %H:%M:%S')\n",
    "elif scen=='rcp45' or scen =='rcp85':\n",
    "    start_date = \"20060101\" # Used for downloading data\n",
    "    end_date   = \"21000101\" # Used for downloading data\n",
    "    startDates = pd.to_datetime(['2006-01-01 00:00:00','2010-01-01 06:00:00','2020-01-01 06:00:00','2030-01-01 06:00:00','2040-01-01 06:00:00','2050-01-01 06:00:00','2060-01-01 06:00:00','2070-01-01 06:00:00','2080-01-01 06:00:00','2090-01-01 06:00:00'],format='%Y-%m-%d %H:%M:%S')\n",
    "    endDates   = pd.to_datetime(['2010-01-01 00:00:00','2020-01-01 00:00:00','2030-01-01 00:00:00','2040-01-01 00:00:00','2050-01-01 00:00:00','2060-01-01 00:00:00','2070-01-01 00:00:00','2080-01-01 00:00:00','2090-01-01 00:00:00','2100-01-01 00:00:00'],format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "workingDir=os.getcwd()\n",
    "workingDir=workingDir+'/'\n",
    "\n",
    "#####################################################\n",
    "############## NO NEED TO CHANGE BELOW ############## \n",
    "#####################################################\n",
    "\n",
    "# Check if we've already created a directory for subsetted/processed data\n",
    "outDir = '/glade/scratch/currierw/'+model+'/'+scen+'/' # must have back slash\n",
    "if os.path.isdir(outDir) == False : # Make output directory if it doesn't exist\n",
    "    os.makedirs(outDir)\n",
    "    print(\"created out directory: \"+outDir)\n",
    "else:\n",
    "    print(\"Out directory already exists: \"+outDir)\n",
    "\n",
    "# See if data exists on Cheyenne already\n",
    "# Note: .txt file is important for figuring this out - just searches .txt function\n",
    "os.chdir(workingDir)\n",
    "cmd='find /glade/collections/cmip/cmip5/output1/'+ins+'/'+model+'/'+scen+'/6hr/atmos/6hrLev/'+ensemble+'/*/* -type d > CMIPcheyene.txt'\n",
    "subprocess.call(cmd, shell=True)\n",
    "fDirs,varDir,varFiles = cmipFunctions.searchChyColl(workingDir+\"CMIPcheyene.txt\",model,'ta') # var not important here, use fDirs\n",
    "if len(fDirs)==0:\n",
    "    print('Not on Cheyenne: Need to download all variables')\n",
    "    chyColl     = False         # Data exist in cheyenne collection already - see list\n",
    "    inDir       = outDir\n",
    "else:\n",
    "    print('Data are already on Cheyenne! Yay!')\n",
    "    chyColl     = True         # Data exist in cheyenne collection already - see list\n",
    "    inDir       = os.path.dirname(fDirs[-1])+'/' # need backward slash at end\n",
    "    print(\"Cheyenne data are located here: \"+inDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF FILES EXIST\n",
    "# cmipFunctions.checkFiles(inDir,'hus',scen,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'ps',scen,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'ta',scen,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'ua',scen,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'va',scen,chyColl)\n",
    "# cmipFunctions.convertTos(inDir,outDir,Model,scen,chyColl)\n",
    "# cmipFunctions.checkFiles(inDir,'tos',scen,chyColl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orogDir='/glade/collections/cmip/cmip5/output1/'+ins+'/'+model+'/'+scen+'/fx/atmos/fx/r0i0p0/latest/orog/*.nc'\n",
    "try:    # Try opening a file that already exists on cheyenne\n",
    "    orogDS=xr.open_mfdataset(orogDir,combine='by_coords')\n",
    "except:\n",
    "    os.chdir(outDir)\n",
    "    orogFile=sorted(glob.glob(outDir+'orog*.nc')) # Get list of files already been downloaded\n",
    "    if len(orogFile) == 0:              # If files hasn't been downloaded - download them\n",
    "        if model ==  'bcc-csm1-1-m': # Orography isn't stored for bcc-csm1-1-m in historical - uses /piControl/fx/atmos/fx/r0i0p0/latest/orog/\n",
    "            get_dataset.download(model='bcc-csm1-1-m', run='r0i0p0', scenario='piControl',start_time=start_date, end_time=end_date, var_name=\"orog\", domain = \"atmos\", interval=\"fx\")\n",
    "        else:\n",
    "            get_dataset.download(model=model, run='r0i0p0', scenario=scen,start_time=start_date, end_time=end_date, var_name=\"orog\", domain = \"atmos\", interval=\"fx\")\n",
    "        orogDS=xr.open_mfdataset(outDir+'orog_fx*.nc',combine='by_coords')\n",
    "    else:\n",
    "        orogDS=xr.open_mfdataset(orogFile[0],combine='by_coords')\n",
    "\n",
    "orogDsSub=orogDS.sel(lat=slice(*lat_bnds), lon=slice(*lon_bnds))\n",
    "orogDS.to_netcdf(outDir+'tmp.nc') # to make it easier just write out this for regridding SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted atmospheric temperature data already existed\n"
     ]
    }
   ],
   "source": [
    "####### TEMPERATURE\n",
    "# Load in Temperature Data First\n",
    "taFiles=glob.glob(outDir+'ta*.nc') # Get list of files already subsetted\n",
    "if chyColl == False and len(taFiles) == 0:\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=model,var_name=\"ta\",domain=\"atmos\",interval=\"6hr\", run=ensemble,                                                            scenario=scen, start_time=start_date, end_time=end_date)\n",
    "    cmipFunctions.loadNonStaggeredVars(outDir,outDir,'ta',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "\n",
    "taFilesSub=glob.glob(outDir+'ta*subset.nc') # Get list of files already subsetted\n",
    "if len(taFilesSub) == 0:              # If list of files haven't been subsetted, subset them\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'ta',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "else:\n",
    "    print(\"Subsetted atmospheric temperature data already existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted sea surface temperature data already existed\n"
     ]
    }
   ],
   "source": [
    "####### SEA SURFACE TEMEPRATURE\n",
    "\n",
    "# daily, historical, rcp45, rcp85, sea surface temperature data doesn't exist on cheyenne\n",
    "# download first - except for NCAR/CCSM4 and NSF-DOE-NCAR-CESM1-WACCM\n",
    "\n",
    "tosFiles=glob.glob(outDir+'tos*.nc') # Get list of files already subsetted\n",
    "tosChyDir='/glade/collections/cmip/cmip5/output1/'+ins+'/'+model+'/'+scen+'/day/ocean/day/'+ensemble+'/latest/tos/'\n",
    "tosChyFiles=sorted(glob.glob(tosChyDir+'*.nc'))\n",
    "if len(tosFiles) == 0 :              # If list of files haven't been subsetted/downlaoded - check to see if they're on Cheyenne\n",
    "    if len(tosChyFiles) == 0:        # If files don't exist on Cheyenne download them\n",
    "        os.chdir(outDir)\n",
    "        get_dataset.download(model=model,var_name=\"tos\",domain=\"ocean\",interval=\"day\", run=ensemble,\\\n",
    "                             scenario=scen, start_time=start_date, end_time=end_date)\n",
    "\n",
    "# Convert the TOS data from rotated pole to temperature data\n",
    "tosFilesRegrd=glob.glob(outDir+'tos_day*_regrd.nc')\n",
    "if len(tosFilesRegrd) == 0:\n",
    "    if len(tosChyFiles) > 0: # if TOS files are on cheyenne - inDir = tosChyDir\n",
    "        os.chdir(outDir)\n",
    "        cmipFunctions.convertTos(outDir,tosChyDir)\n",
    "    else: # we downloaded the data: inDir = outDir\n",
    "        os.chdir(outDir)\n",
    "        cmipFunctions.convertTos(outDir,outDir)\n",
    "# Convert the\n",
    "tosFilesSub=glob.glob(outDir+'tos_6hr*_regrd_subset.nc')\n",
    "if len(tosFilesSub) == 0:\n",
    "    cmipFunctions.processTOS(outDir,outDir,'tos',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates)\n",
    "else:\n",
    "    print(\"Subsetted sea surface temperature data already existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetted specific humidity data already existed\n"
     ]
    }
   ],
   "source": [
    "husFiles=glob.glob(outDir+'hus*.nc')\n",
    "if chyColl == False and len(husFiles)==0:\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=model,var_name=\"hus\",domain=\"atmos\",interval=\"6hr\", run=ensemble,\\\n",
    "                         scenario=scen, start_time=start_date, end_time=end_date)\n",
    "    cmipFunctions.loadNonStaggeredVars(outDir,outDir,'hus',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "\n",
    "husFilesSub=glob.glob(outDir+'hus*subset.nc')\n",
    "if len(husFilesSub) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'hus',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "else:\n",
    "    print(\"Subsetted specific humidity data already existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uFiles=sorted(glob.glob(outDir+'ua*.nc'))\n",
    "len(uFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are not staggered\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_19500101-19600101_r1i1p1_subset.nc\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_19600101-19700101_r1i1p1_subset.nc\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_19700101-19800101_r1i1p1_subset.nc\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_19800101-19900101_r1i1p1_subset.nc\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_19900101-20000101_r1i1p1_subset.nc\n",
      "Writing out /glade/scratch/currierw/GFDL-CM3/historical/ua_6hrLev_GFDL-CM3_20000101-20060101_r1i1p1_subset.nc\n",
      "Downloading: va_6hrLev_GFDL-CM3_historical_r1i1p1_1950010100-1950123123.nc\n",
      "Downloading: va_6hrLev_GFDL-CM3_historical_r1i1p1_1951010100-1951123123.nc\n"
     ]
    }
   ],
   "source": [
    "###### U Data\n",
    "uFiles=sorted(glob.glob(outDir+'ua*.nc'))\n",
    "if chyColl == False and len(uFiles)==0: # if files aren't on cheynne and they don't exist\n",
    "    os.chdir(outDir)\n",
    "    # Download Files\n",
    "    get_dataset.download(model=model,var_name=\"ua\",domain=\"atmos\",interval=\"6hr\", run=ensemble,\\\n",
    "                         scenario=scen, start_time=start_date, end_time=end_date)\n",
    "    # Load the files and process them\n",
    "    cmipFunctions.loadStaggeredVars(outDir,outDir,'ua',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "\n",
    "# if files haven't been subsetted (exist on cheyenne or downloaded but not processed) - proccess those files\n",
    "uFilesSub=sorted(glob.glob(outDir+'ua*subset.nc')) \n",
    "if len(uFilesSub) == 0:\n",
    "    cmipFunctions.loadStaggeredVars(inDir,outDir,'ua',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "else:\n",
    "    print(\"Subsetted U wind speed data already existed\")\n",
    "\n",
    "###### V Data\n",
    "vFiles=glob.glob(outDir+'va*.nc')\n",
    "if chyColl == False and len (vFiles)==0:\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=model,var_name=\"va\",domain=\"atmos\",interval=\"6hr\", run=ensemble,\\\n",
    "                         scenario=scen, start_time=start_date, end_time=end_date)\n",
    "    cmipFunctions.loadStaggeredVars(outDir,outDir,'va',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "\n",
    "vFilesSub=glob.glob(outDir+'va*subset.nc')\n",
    "# if len(vFiles) > 0 and len(vFilesSub) == 0: # if files were already downloaded - use them\n",
    "#     cmipFunctions.loadStaggeredVars(outDir,outDir,'va',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "if len(vFilesSub) == 0:\n",
    "    cmipFunctions.loadStaggeredVars(inDir,outDir,'va',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "else:\n",
    "    print(\"Subsetted V wind speed data already existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Surface Pressure\n",
    "psFiles=glob.glob(outDir+'ps*.nc')\n",
    "if chyColl == False and len(psFiles)==0:\n",
    "    os.chdir(outDir)\n",
    "    get_dataset.download(model=model,var_name=\"ps\",domain=\"atmos\",interval=\"6hr\", run=ensemble,\\\n",
    "                         scenario=scen, start_time=start_date, end_time=end_date)\n",
    "    cmipFunctions.loadNonStaggeredVars(outDir,outDir,'ps',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "\n",
    "psFilesSub=glob.glob(outDir+'ps*subset.nc')\n",
    "if len(psFilesSub) == 0:\n",
    "    cmipFunctions.loadNonStaggeredVars(inDir,outDir,'ps',model,scen,ensemble,lat_bnds,lon_bnds,startDates,endDates,chyColl)\n",
    "else:\n",
    "    print(\"Subsetted surface pressure data already existed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Precipitation\n",
    "# daily, historical, rcp45, rcp85, sea surface temperature data doesn't exist on cheyenne - download first\n",
    "# prcFiles=glob.glob(outDir+'prc*.nc')\n",
    "# if len(prcFiles) == 0 :\n",
    "#     os.chdir(outDir)\n",
    "#     get_dataset.download(model=Model,var_name=\"prc\",domain=\"atmos\",interval=\"3hr\", run=ensemble, scenario=time_period, start_time=start_date, end_time=end_date)\n",
    "\n",
    "# start = ['1960-01-01 06:00:00','1970-01-01 06:00:00','1980-01-01 06:00:00','1990-01-01 06:00:00']\n",
    "# end   = ['1970-01-01 00:00:00','1980-01-01 00:00:00','1990-01-01 00:00:00','2000-01-01 00:00:00']\n",
    "# startDatesPrc = pd.to_datetime(start, format='%Y-%m-%d %H:%M:%S')\n",
    "# endDatesPrc   = pd.to_datetime(end,   format='%Y-%m-%d %H:%M:%S')\n",
    "# # Precipitation is 3D, only time,lat,lon - stored in longer time files in 3 hr data\n",
    "# prcFiles=glob.glob(outDir+'prc*subset.nc')\n",
    "# if len(prcFiles) == 0:\n",
    "#     cmipFunctions.loadNonStaggeredVarsResample(outDir,outDir,'prc',Model,time_period,ensemble,lat_bnds,lon_bnds,startDatesPrc,endDatesPrc,False,'6hr','sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsT    = xr.open_mfdataset(outDir+'ta*subset.nc',combine='by_coords')\n",
    "dsTOS  = xr.open_mfdataset(outDir+'tos*subset.nc',combine='by_coords')\n",
    "dsHus  = xr.open_mfdataset(outDir+'hus*subset.nc',combine='by_coords')\n",
    "dsU    = xr.open_mfdataset(outDir+'ua*subset.nc',combine='by_coords')\n",
    "dsV    = xr.open_mfdataset(outDir+'va*subset.nc',combine='by_coords')\n",
    "# dsPrc  = xr.open_mfdataset(outDir+'prc*subset.nc',combine='by_coords')\n",
    "# dsPrcO = xr.open_mfdataset(outDir+'prc_3hr*.nc',combine='by_coords')\n",
    "dsPs   = xr.open_mfdataset(outDir+'ps*subset.nc',combine='by_coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the model level heights [m] or Pressure Levels\n",
    "try:\n",
    "    z_t=dsT.lev+(dsT.b*dsT.orog)\n",
    "    print(\"\\nCalculating model Level Heights\")\n",
    "except:\n",
    "    P = dsT['a']*dsT['p0'] + dsT['b']*dsT['ps']\n",
    "    print(\"Computed pressure from surface pressure\")\n",
    "    pressure  = True\n",
    "    elevation = False\n",
    "else:\n",
    "    elevation = True\n",
    "    pressure  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate water vapor mixing ratio (w/Qv) [kg/kg] from specific humdiity\n",
    "print(\"\\n Converting from Specific Humidity to Water Vapor Mixing Ratio, Qv = hus/1-hus\")\n",
    "dsHus['Qv'] = dsHus['hus']/(1-dsHus['hus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating New Dataset\n",
    "print(\"\\nCreating new dataset\\n\")\n",
    "\n",
    "# Create New Datasets that's just the datasets with dimensions and coordinates\n",
    "\n",
    "######## Terrain Height\n",
    "dsOrog = xr.Dataset({\"HGT\":((\"lat\",\"lon\"),orogDsSub.orog)},coords={\"lat\":orogDsSub.lat,\"lon\":orogDsSub.lon})\n",
    "dsOrog['HGT'].attrs['standard_name'] = orogDsSub['orog'].standard_name\n",
    "dsOrog['HGT'].attrs['long_name']     = orogDsSub['orog'].long_name\n",
    "dsOrog['HGT'].attrs['units']         = orogDsSub['orog'].units\n",
    "dsOrog['HGT'].attrs['Processing Note'] = 'Provided orogoraphy file - orogoraphy variable'\n",
    "print('Loaded the orography data')\n",
    "\n",
    "if elevation:\n",
    "    ######## 3D Model Level Heights [m]\n",
    "    ds_Z    = xr.Dataset({\"Z\":((\"lev\",\"lat\",\"lon\"),z_t)},\n",
    "                 coords={\"lev\":tads_sub.lev,\"lat\":tads_sub.lat,\"lon\":tads_sub.lon})\n",
    "    ds_Z['Z'].attrs['standard_name'] = '3D model level heights'\n",
    "    ds_Z['Z'].attrs['long_name'] = '3D Model Level Heights'\n",
    "    ds_Z['Z'].attrs['units'] = 'm' \n",
    "    ds_Z['Z'].attrs['Processing Note'] = 'Calculated using temperature lev coordinates, b values, and orogoraphy varaibles. Z = lev + b * orog' \n",
    "    print('Loaded the model level elevations')\n",
    "if pressure:\n",
    "    ds_P = xr.Dataset({\"P\":((\"time\",\"lev\",\"lat\",\"lon\"),P)},\n",
    "                 coords={\"time\":dsT.time,\"lev\":P.lev,\"lat\":P.lat,\"lon\":P.lon})\n",
    "    ds_P['P'].attrs['standard_name'] = 'pressure' \n",
    "    ds_P['P'].attrs['long_name'] = 'pressure' \n",
    "    ds_P['P'].attrs['comment'] = 'calculated from subsetted 6 hr temperature data file: a*p0+b*Ps' \n",
    "    ds_P['P'].attrs['units'] = 'Pa' \n",
    "    print('Loaded the 4D pressure data')\n",
    "\n",
    "\n",
    "######## Surface Air Pressure [Pa]\n",
    "ds_Ps   = xr.Dataset({\"Ps\":((\"time\",\"lat\",\"lon\"),dsPs.ps)},\n",
    "                 coords={\"time\":dsPs.time,\"lat\":dsPs.lat,\"lon\":dsPs.lon})\n",
    "ds_Ps['Ps'].attrs = dsPs.ps.attrs\n",
    "print('Loaded the surface air pressure data')\n",
    "\n",
    "######## Sea Surface Temeperature [K]\n",
    "ds_SST  = xr.Dataset({\"SST\":((\"time\",\"lat\",\"lon\"),dsTOS.tos)},\n",
    "                 coords={\"time\":dsTOS.time,\"lat\":dsT.lat,\"lon\":dsT.lon})\n",
    "ds_SST['SST'].attrs = dsTOS['tos'].attrs\n",
    "ds_SST['SST'].attrs['Processing Note'] = 'Daily data forward filled to 6H data' \n",
    "print('Loaded the sea surface temperature data')\n",
    "\n",
    "######## Precipitation [kg m-2 s-1]\n",
    "# ds_prec = xr.Dataset({\"prec\":((\"time\",\"lat\",\"lon\"),dsPrc.prc)},\n",
    "#                  coords={\"time\":dsPrc.time,\"lat\":dsPrc.lat,\"lon\":dsPrc.lon})\n",
    "# ds_prec['prec'].attrs['standard_name'] =dsPrcO['prc'].attrs['standard_name']\n",
    "# ds_prec['prec'].attrs['long_name'] = dsPrcO['prc'].attrs['long_name']\n",
    "# ds_prec['prec'].attrs['comment'] = 'at surface. This is a 6-hour mean convective preciptiation'\n",
    "# ds_prec['prec'].attrs['units'] = dsPrcO['prc'].attrs['units']\n",
    "# ds_prec['prec'].attrs['cell_methods'] = dsPrcO['prc'].attrs['cell_methods']\n",
    "# ds_prec['prec'].attrs['cell_measures'] = dsPrcO['prc'].attrs['cell_measures']\n",
    "# ds_prec['prec'].attrs['associated_files'] = dsPrcO['prc'].attrs['associated_files']\n",
    "# ds_prec['prec'].attrs['Processing Note'] = 'Resample from 3H data to 6H data using the sum between time-steps'\n",
    "\n",
    "######## Air Temperature [K[]]\n",
    "ds_T = xr.Dataset({\"T\":((\"time\",\"lev\",\"lat\",\"lon\"),dsT.ta)},\n",
    "                  coords={\"time\":dsT.time,\"lev\":dsT.lev,\"lat\":dsT.lat,\"lon\":dsT.lon})\n",
    "ds_T['T'].attrs = dsT.ta.attrs\n",
    "print('Loaded the air temperature data')\n",
    "\n",
    "######## Water Vapor Mixing Ratio [kg/kg]\n",
    "ds_Qv = xr.Dataset({\"Qv\":((\"time\",\"lev\",\"lat\",\"lon\"),dsHus.Qv)},\n",
    "                   coords={\"time\":dsHus.time,\"lev\":dsHus.lev,\"lat\":dsHus.lat,\"lon\":dsHus.lon})\n",
    "ds_Qv['Qv'].attrs['standard_name'] = 'Water Vapor Mixing Ratio'\n",
    "ds_Qv['Qv'].attrs['long_name'] = 'Water Vapor Mixing Ratio'\n",
    "ds_Qv['Qv'].attrs['units'] = 'kg/kg'\n",
    "ds_Qv['Qv'].attrs['cell measures'] = dsHus['hus'].attrs['cell_measures']\n",
    "ds_Qv['Qv'].attrs['associated_files'] = dsHus['hus'].attrs['associated_files']\n",
    "ds_Qv['Qv'].attrs['Processing Note'] = 'Calculated from specific humidity (q) data Qv = q/1-q'\n",
    "print('Loaded the water vapor mixing ratio data')\n",
    "\n",
    "######## North South Wind Speeds [m s-1]\n",
    "ds_v = xr.Dataset({\"V\":((\"time\",\"lev\",\"lat\",\"lon\"),dsV.va)},\n",
    "                  coords={\"time\":dsV.time,\"lev\":dsT.lev,\"lat\":dsV.lat,\"lon\":dsV.lon})\n",
    "ds_v['V'].attrs = dsV.va.attrs\n",
    "ds_v['V'].attrs['Processing Note'] = 'Interpolated/Regrided from staggered north-south grid - one additional row offset by 1/2 a grid cell in latitude to the temperature grid'\n",
    "print('Loaded the V wind speed data')\n",
    "\n",
    "######## East Wind Wind Speeds [m s-1]\n",
    "ds_u = xr.Dataset({\"U\":((\"time\",\"lev\",\"lat\",\"lon\"),dsU.ua)},\n",
    "                  coords={\"time\":dsU.time,\"lev\":dsT.lev,\"lat\":dsU.lat,\"lon\":dsU.lon})\n",
    "ds_u['U'].attrs = dsU.ua.attrs\n",
    "ds_u['U'].attrs['Processing Note'] = 'Interpolated/Regrided from staggered east-west grid - one additional column offset by 1/2 a grid cell in longitude to the temperature grid'\n",
    "print('Loaded the U wind speed data')\n",
    "\n",
    "print('Merging the dataset')\n",
    "# Make a new dataset\n",
    "if elevation:\n",
    "    ds = xr.merge([dsOrog, ds_Z, ds_Ps, ds_SST, ds_T, ds_Qv, ds_v, ds_u])\n",
    "elif pressure:\n",
    "    ds = xr.merge([dsOrog, ds_P, ds_Ps, ds_SST, ds_T, ds_Qv, ds_v, ds_u])\n",
    "print('Merged dataset - fixing longitudes: 0-359 to -180-179')\n",
    "\n",
    "# Convert longitude coordinates from 0-359 to -180-179\n",
    "ds['lon']=(((ds['lon'] + 180) % 360) - 180)\n",
    "ds['lon'].attrs['units']         = 'degrees_east'\n",
    "ds['lon'].attrs['comment']       = 'LONGITUDE, WEST IS NEGATIVE'\n",
    "ds['lon'].attrs['axis']          = 'X'\n",
    "ds['lon'].attrs['long_name']     = 'longitude'\n",
    "ds['lon'].attrs['standard_name'] = 'longitude'\n",
    "\n",
    "ds['SST']=ds['SST'].ffill('time')\n",
    "print('Filling in SST if any NaNs')\n",
    "\n",
    "now = datetime.now()\n",
    "ds.attrs['Condensed/Merged File Created'] = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "ds.attrs = dsT.attrs\n",
    "print(\"Created new dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory - if necessary\n",
    "if os.path.isdir(outDir+'forcing/') == False:\n",
    "    print(\"Writing new directory: \"+outDir+'forcing/')\n",
    "    os.mkdir(outDir+'forcing/')\n",
    "\n",
    "# Create a new \n",
    "allFiles=glob.glob(outDir+'forcing/'+model+'_6hrLev_'+ scen +'_'+ ensemble +'*' +'_subset.nc')\n",
    "if len(allFiles) == 0:\n",
    "    print(\"Writing out merged files to 10 year periods\")\n",
    "    for startDateLs, endDateLs in zip(startDates, endDates):\n",
    "        ds_sub_Time  = ds.sel(time=slice(startDateLs.strftime('%Y-%m-%dT%H:%M:%S'),endDateLs.strftime('%Y-%m-%dT%H:%M:%S')))  # Time slice\n",
    "        ds_sub_Time.to_netcdf(outDir+'forcing/'+model+'_6hrLev_'+ scen +'_'+ ensemble +'_' + startDateLs.strftime('%Y%m%d')+ '-'+endDateLs.strftime('%Y%m%d') +'_subset.nc')\n",
    "else:\n",
    "    print(\"Files already existed in forcing directory - did not overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate virtual temperature\n",
    "# e=0.622 # ratio of the gas constants of air and water vapor\n",
    "# dsSub['tv']=dsSub['T']*((1+dsSub['Qv']/e)/(1+dsSub['Qv'])) # Qv is the mixing ratio\n",
    "# Rd = 287 # J K-1 kg-1 - gas constant for dry air\n",
    "# g=9.81 # m s-1 - gravity\n",
    "# #Hypsometric equation - relating thickness to virtual temperature\n",
    "# dsSub['Z']=((Rd*dsSub['tv'])/g)*xr.ufuncs.log(dsSub['Ps']/dsSub['P'])+dsSub['HGT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = ds['T'][0,0,:,:].plot(x='lon', y='lat',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()})\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsWRF=xr.open_dataset('/glade/work/gutmann/crb/default_icar_test_run/forcing/wrfout_d01_2000-10-01_00:00:00')\n",
    "dsWRF.XLONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsBase=xr.open_dataset('/glade/work/gutmann/crb/default_icar_test_run/baseline/headwaters_4km.nc')\n",
    "dsBase.XLONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = ds['HGT'][:,:].plot(x='lon', y='lat',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()},cmap='terrain',vmin=0, vmax=3500)\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = dsWRF['HGT'][0,:,:].plot(x='XLONG', y='XLAT',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()},cmap='terrain',vmin=0, vmax=3500)\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "p = dsBase['HGT'][0,:,:].plot(x='XLONG', y='XLAT',transform=ccrs.PlateCarree(),subplot_kws={'projection': ccrs.PlateCarree()},cmap='terrain',vmin=0, vmax=3500)\n",
    "ax.coastlines();ax.gridlines();ax.add_geometries(states_gdf.geometry, crs = ccrs.PlateCarree(),facecolor='none', edgecolor='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
